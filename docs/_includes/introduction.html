<!-- Services Section -->
    <section id="introduction">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Introduction and Context</h2>
                    <h3 class="section-subheading text-muted">Automatic Playlist Continuation (APC) is an interesting and modern challenge, for platforms providing the user with choices from a large pool of content. Whether it is movie clips, music or other digital content, APC needs to thread the needle between providing users with content from their own playlist, as well as providing fresh but fitting choices to expand their world. In the world of music, APC based on song features has long been introduced. However, adding both song sentiments as well as userâ€™s sentiments to connect even better with the current mood of the user in selecting songs remains a challenge. </h3>
                </div>
                    <p> For this project, we chose to tackle the cold start problem for our project and generate a playlist of songs given a short amount of songs. Our idea was to expand more on the quality of the playlist, and answer the research question: How can we make music recommendation personalizable and automated? Our initial exploratory analysis of the data reshaped the way in which we looked at analysis. We then refined our research question based on the use of additional audio features from Spotify and the EDA to encompass: Are we able to quantify music based on song features allowing for more personalized automated music recommendations?

                    While our models are trained with all the features, for the purposes of this project, we chose to place emphasis on the danceability, energy, valence, and specchiness features for personalizing the recommendations, considering that users would choose to have variability in aspects such as tempo.</p>

                    <p>This problem is challenging due to the design choices that need to be carefully selected in building such a system. In certain cases, intution may disagree with the story the data is telling us - it's important to differentiate between cases where we should utilize <b>data-centered design</b> (e.g. output tracks with the highest cosine similarity and call it a day) or <b>human-centered design</b> (e.g. ignore cosine similarity and focus on aspects of the tracks humans are more likely to pay attention to, like how danceable the songs in their playlist are). The goal of this project is to focus on the latter</p>

                    <p><b>Extending on this, from the research we performed for a literature review, we identified that user input is critical for generating rich and relevant recommendations. Therefore, as part of the scope of this project, we wanted to focus on the user interface (UI) of our system in addition to the models as a prototype of a real-world application. As we will explore in later sections, we were able to incorporate intuitive features into our UI that enhances the user experience, such as dynamically reconfiguring the default answers of the quiz the user takes to match what the system initially hypothesize the user's preferences are. </b></p>

                    <b>We utilized the Million Playlists dataset provided by the teaching staff, which will be explained in detail in the next section.</b>
                    <p>Technologies utilized as part of the project:</p>
                    <b>Data Formats</b>
                    <ul>
                        <li>Parquet</li>
                        <li>CSV</li>
                    </ul>
                    <p>Size of data that was analyzed: 
                    Due to the extreme size of each of the .csv files, we chose to analyze subsets of certain .csv files so as to not overload our computers. We tried using different file formats, such as Parquet, for efficiently visualizing the entire .csv files, although this was leading to an extremely large run time for the visualizations as well.

                        <b>Visualization</b>
                        <ul>
                        <li>Using about 3000 unique songs from the dataset (subset of new_feature_data_songs999.csv in the associated repo)</li>
                        </ul>

                        <b>Recommender System</b>
                        <ul>
                        <li>Using about 42,499 unique songs from the dataset< (new_feature_data_songs999.csv in the associated repo)</li>
                        </ul>

                        <b>Sentiment Analysis Models</b>
                        <ul>
                        <li>Using about 100,000 unique songs from the dataset. The parquet files are of the order of 13 GB, so we could not add this to the repository. The parquet file generation notebook  generates these files from the datset shared with the teaching staff. </li>
                        <li>Using Google Colab with GPU</li>
                        <li>Generated vectors are 768 dimensions, shared as a numpy array (objects are pickled). The generate data is roughly 700 mb for each LM (BERT & GPT-2). Following are the public google drive links for the same:</li>
                        <li><b>BERT part00: </b><a href="https://drive.google.com/file/d/1-2M9CIkX0MtPflfnG9CQQvU0nppyt6Xq">https://drive.google.com/file/d/1-2M9CIkX0MtPflfnG9CQQvU0nppyt6Xq</a></li>
                        <li><b>GPT-2 part00</b><a href="https://drive.google.com/open?id=1VvPbasE3BbMwUg-pHz8eaaicQK0YZOeP">https://drive.google.com/open?id=1VvPbasE3BbMwUg-pHz8eaaicQK0YZOeP</a></li>
                        </ul>

                    </p>

                    <b>Packages Used</b>
                    <ul>
                        <li>sklearn (for the recommender system)</li>
                        <li>matplotlib</li>
                        <li>seaborn</li>
                        <li>spotipy (for data acqusition)</li>
                    </ul>
                    <b>Algorithms Utilized</b>
                    <ul>
                        <li>K-Nearest Neighbors</li>
                        <li>K-Means</li>
                        <li>PCA</li>
                        <li>BERT</li>
                        <li>GPT-2</li>
                    </ul>

                <div class="col-lg-12 text-center">
                    <p>Below, we include a picture of our system's pipeline for inference: </p>
                    <img src="img/Pipeline_For_Inference.jpg" alt="Pipeline for Inference">
                    <br/>
                    <h2 class="section-heading">Preliminary Exploratory Data Analysis</h2>
                    <h3 class="section-subheading text-muted">As part of the EDA section, we cover four sections: Data description, data exploration, in addition to clustering and PCA.</h3>
                </div>

            <h4 class="subheading">Data Description</h4>
            <p class="text-muted">The Million Playlists dataset originally consisted of 9 columns - we added 8 more columns to the dataset corresponding to audio features of the tracks obtained using the Spotify API.</p>


            <ul>
                <li>Pid: The playlist ID</li>
                <li>Pos: The position of the song in the playlist</li>
                <li>Artist_name: The name of the song artist</li>
                <li>Track URI: The URI for the track.</li>
                <li>Artist URI: The URI for the artist.</li>
                <li>Track name: The name of the song/track</li>
                <li>Album URI: The URI for the album</li>
                <li>Duration_ms: The duration in milliseconds</li>
                <li>Album_name: The name of the album</li>
                <li>Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic.</li>
                <li>Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity.</li>
                <li>Liveness: Detects the presence of an audience in the recording.</li>
                <li>Loudness: The overall loudness of a track in decibels (dB).</li>
                <li>Tempo: The overall estimated tempo of a track in beats per minute (BPM).</li>
                <li>Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.</li>
                <li>Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.</li>
                <li>Speechiness: Speechiness detects the presence of spoken words in a track.</li>
                <li>Genre: Work in progress acquisition of genre labels - please see note in below description.</li>
            </ul>
            <p class="text-muted">Each dataset file consists of approximately 67,000 rows of data, and there are approximately 1000 csv files. We registered for and created a credential for our specific application/project.</p>

            </div>
            <div class="row">
            <h4 class="subheading">Data Exploration</h4>
            <p>We present the results we obtained after dropping NAN values from the dataframe.</p>

            <p>For the numerical variables, we noticed that some of the variables have a different order of magnitude compared to the other features. For example, the "Tempo" variable has values ranging from 0 to 220, the "Loudness" variable had values ranging from -42 to approximately 2, and the "danceability" variable has values ranging from 0 to approximately 1. This suggests that scaling the dataset would be useful to account for the features with varying orders of magnitude - which we will consider later.</p>

            <b>Visualizations of data:</b>


            First, let's look at the number of unique artists in the list (there are 8,303). Below we see the top ten artist entries in the datafile. 

            <center><img src="img/viz_1.JPG" alt="Number of unique artists in the list"></center>
            
            <p><b>a.</b> Top ten artist entries in the datafile. <b>b.</b> We see the frequency distribution of the artists: there is a large skew to the data set, with a small number of artists having a large presence, and to the overly large majority being â€˜one-trick-poniesâ€™, having only a single hit song in the database. One hypothesis we can make from these insights is that users would be more open to listening to songs from other artists that aren't (yet) part of their playlist, rather than tracks from the same artists the users listen to, given that most artists have a small presence.</p>


            <p>Now, let's dive deeper into the data. Let's look at a boxplot overview of the data of the songs for 8 features (accousticness, energy, liveness, loudness, tempo, valence, danceability, and speechiness), on the basis of which we will try to categorize songs and predict choice.</p>

            <center><img src="img/viz_2.JPG" alt="Heatmaps, boxplots, and correlation plots"></center>

            <p><b>c.</b> We see most of the distributions are skewed, except for tempo and valence,for example. For examples, there appears to be some skewing in data for liveness, loudness and speechiness. Liveness might be a multimodal distribution of liveness of regular songs, and a secondary group of actual songs for live performances. For speechiness, this might be an effect of music type (e.g. rock/balads versus rap songs). As we'll notice later in the clustering section, the wide range of the distributions for certain features such as danceability will come in handy when data points are assigned to different clusters and we seek seperability of the clusteres for interpretation. By analyzing the box plots, we were able to develop this intution early-on and understand what to expect when moving on to the clustering analysis.
            <b>d.</b> Next, we inspect a correlation matrix for the features to see if we have any issues with overlapping features or risk multicollinearity in further analysis. We only see particularly high correlations between acousticness and energy and acousticness and loudnness, although in other cases the other features don't demonstrate high multicollinearity. Let's look in this detail. <b>e.</b> We utilized the correlation plots for four features in particular. Overall there seems to be a negative correlation for accousticness and energy (e.g. acoustic songs are more often low-energy), a positive relationship between loudness and energy (more energetic songs are louder). Valance and danceability are positively correlated, as are energy and valence. Since we see interesting relationships between these variables that may be encoding musical sensabilities, we chose to keep all the features in the dataset and see how they inform our analysis. </p>

            </div>
            <div class="row">
            <h4 class="subheading">Unsupervised Learning: Clustering and PCA</h4>
            <p class="text-muted">Below, we present our results for k-means clustering, after scaling the dataset using Standard Scaler and training and fitting the K-means model on the data</p>

            <p><b>f.</b> We clustered the data using K-means with 3 clusters - below, we demonstrate the distribution of the data points across the 3 clusters - we see majority of the data points have been assigned to Cluster 0. <b>g.</b> It is challenging to interpret the results of unsupervised learning algorithms, given there are no labels. We can use one approach where we analyze the distribution of values with respect to the K-means clusters. Therefore, we found analyzing the distribution of values with respect to the K-means clusters would be helpful. We see the distributions for the clusters seem to overlap a lot, but we can make out a few trends.  For example, it seems that Cluster 0 contains data points with medium acousticness, medium energy, high valence, and high danceability. Cluster 1 seems to contain data points that have high acousticness, low energy, low valence, and medium danceability. Cluster 2 contains data points with medium/low acousticness, high energy, medium valence, and medium danceability. For variables such as "speechiness", it is harder to identify the trends, although we can generally say that Cluster 0 contains data points with relatively higher speechiness, Cluster 1 contains data points with low speechiness, and Cluster 2 contains data points with medium speechiness. </p>

            <center><img src="img/viz_3.JPG" alt="K-means clustering results"></center>

            <p>Given that even after scaling the data, the order of the rows remains the same, we can inspect which songs (from the original dataframe that still contains the categorical variables and the original values of the featurs) were assigned to what Clusters:</p>

            <center><img src="img/tracks_cluster_assignment.JPG" alt="Tracks belonging to K-means clusters"></center>
            <p>We see in this case, "Give 'Em a Chance" was assigned to Cluster 0 - we do see it has low acousticness, relatively high energy, high valence and high danceability which matches our interpretation for Cluster 0.

            "River Flows In You" was assigned to Cluster 1 - we see high acousticness, low energy, etc. which matches our interpretation for Cluster 1.

            "Requiem For A Tower" was assigned to Cluster 2 - we see medium/low acousticness, high energy, etc. which matches our interpretation for Cluster 2.

            In this case, we have only cluster (Cluster 0) that represents happy tracks. We experimented with different numbers of clusters, and observed similar observations in terms of seperability of features even with more clusters, so we decided to choose three clusters and move forward.</p>


            With the insights gathered from EDA, we started our implementation.
            </div>
            </div>
    </section>