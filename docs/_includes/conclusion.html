<!-- Services Section -->
    <section id="conclusion">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Conclusion</h2>
                </div>


            <b>Strengths of project: </b> <p>

                We scoped and built a full end-to-end pipeline, consisting of a user interface for incorporating feedback from the user, integrating a recommender system model that utilizes unsupervised learning to supplement its predictions and sentiment analysis to generate recommendations. In terms of the user interface, our idea was to have the user take a short quiz on their preferred audio features, and have the clustering algorithm dynamically reconfigure the default answers of the quiz to match what we think the user's preferences, which enhances the user experience. 

            We worked extensively on generating and comparing the results between our recommender system model in addition to the different sentiment analysis models we tried (BERT and GPT-2).  We covered the impact of our model in the Intepretation section, and do not repeat it here, to avoid redundancy.


        </p>

            <b>Short-comings: </b> <p>Future work would include expanding on the implementations we needed to discard during our project trajectory, such as more work on deciding the rankings of the songs. We would approach this by using a linear or logistic regression model trained on a PCA-transformed version of the dataset to predict the position of the track in the playlist. Ranking the recommendations further using the values of the audio features (e.g. rank features from high to low danceability) is another option we would pursue given more time.

            Additionally, using recommender system models other than the type of model used in this project, such as collaborative filtering models, might lead to better performance on the validation and test data. We would approach this aspect by using the Spotipy API to retrieve ratings/likes about the songs as input to the collaborative filtering model.

            Expanding on the implementation of metrics such as R-precision and Normalized discounted cumulative gain (NDCG)
             by addressing the questions we noted above (in the Project Trajectory section) given more time would helps us quantitatively compare the results between our models. As we noted above, this may not be representative of the real-world - for example, for the R-precision metric, defining "relevancy" of a track is vague (users might be more open to tracks that aren't necessarily part of the dataset we used). This is where we see the contention between data-centered design and human-centered design, and for this project, we strove to consider the latter.

            </p>
            <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Thank you!</h2>
            </div>
            </div>
            </div>
    </section>